---
title: "Plotting Functions With The Estimation Package"
output: github_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-"
  
)
```
#A Single Interval Isn't Enough!

Interval estimates such as  ~~confidence~~ **compatibility** intervals are now widely reported. Many journals and reviewers require them alongside the exact P-value of a statistical test and the point estimate. 

While this is a large improvement over what constituted statistical reporting a decade ago, it is still largely inadequate.

Take for example, the 95% compatibility interval. As many have stated before, there is nothing special about **95%**, yet we rarely see intervals of any other level. Choosing to compute a 95% interval is as mindless as choosing a 5% alpha level for hypothesis testing. A single compatibility interval is only a _slice_ of a wide range of compatibility intervals at different levels. Reporting 95% intervals only promotes [cargo-cult statistics](https://rss.onlinelibrary.wiley.com/doi/10.1111/j.1740-9713.2018.01174.x) since there is not much thought behind the choice. [1] 

> "**_Cargo-cult statistics_** – _the ritualistic miming of statistics rather than conscientious practice_."
>
> `r tufte::quote_footer('- Stark & Saltelli, 2018')`

Thus, we propose that instead of only calculating _**one**_ interval estimate, _**every**_ interval associated with a compatibility level be calculated and plotted to form a function. [2-4] 

We will show how to calculate and plot such functions with the <span style="color:#d46c5b">[**estimation**](https://github.com/Zadchow/estimation)</span> package in R.   

# Using Mean Differences

If we were to simulate some random data from a normal distribution with the following code,

```{r}
GroupA<-rnorm(15)
GroupB<-rnorm(15)

RandomData<-data.frame(GroupA, GroupB)
```

and compare the means of these two groups using Welch’s t-test, 

```{r}
testresults<-t.test(GroupA, GroupB, data=RandomData, paired=FALSE)
```

we would likely see some differences, given that we have such a small sample in each group. We can graph our data to see what the variability looks like. 

```{r include=FALSE}
library(tidyr)
RandomDataLong <- gather(data = RandomData, 
             key = Group, 
             value = Estimate)
```

Plotted with a dot plot, we can see there is some notable variability. 
```{r echo=FALSE}
library(ggplot2)
ggplot(RandomDataLong, aes(x=Group, y=Estimate, fill=Group)) + 
  geom_boxplot(fill = "lightgrey")+
  geom_dotplot( binwidth = 0.27, binaxis='y', stackdir='center')+
    theme_light() +
    theme(axis.title.x = element_text(size=13),
          axis.title.y = element_text(size=13),
          legend.text = element_text(size=13)) 


testresults
```

We can see our P-value for the statistical test along with the computed 95% interval (which is given to us by default by the program). Thus, effect sizes that range from the lower bound of this interval to the upper bound are compatible with the test model at this compatibility level. 

However, as stated before, a 95% interval is only an artifact of the commonly used 5% alpha level for hypothesis testing and is nowhere near as informative as a function. 

If we were to take the information from this data and calculate a P-value function where every single compatibility interval and its corresponding P-value were plotted, we would be able to see the full range of effect sizes compatible with the test model at various levels.  

It is relatively easy to produce such a function using the <span style="color:#d46c5b">[**estimation**](https://github.com/Zadchow/estimation)</span> package in R. 

> The current version of **ggplot2** (3.1.0), which this package **depends** on, has a bug in the _sec_axis_ function, that prevents the <span style="color:#d46c5b">[**estimation**](https://github.com/Zadchow/estimation)</span> package from plotting functions correctly. Thus, the package depends on a version of **ggplot2** prior to this (at least until it gets patched in the next version). We recommend installing **ggplot2** v. 3.0.0. Below is the full code to install that version of **ggplot2** and our package. 

> <span style="color:#d46c5b">**library(devtools)**</span>

> <span style="color:#d46c5b">**install_version("ggplot2", version = "3.0.0", repos = "http://cran.us.r-project.org")**</span>

Install the estimation package. Do NOT update to the recent version of **ggplot2** (3.1.0) if prompted. 

> <span style="color:#d46c5b">**install_github("zadchow/estimation")**</span>


We'll use the same data from above to calculate a P-value function and since we are focusing on mean differences using a t-test, we will use the _**meanintervals**_ function to calculate our compatibility intervals function and store it in a dataframe.  

```{r}
library(estimation)
intervalsdf<-meanintervals(GroupA, GroupB, 
                           data=RandomData, method="default")
```
Now thousands of compatibility intervals at various levels have been stored in the dataframe "intervalsdf."                         

We can plot this data using the _**plotpint**_ function (which stands for plot P-value intervals). 

```{r fig.height=6, fig.width=7}
pfunction<-plotpint(intervalsdf)
pfunction
```

Now we can see every compatibility interval and its corresponding P-value and compatibility level plotted. As stated before, a single 95% compatibility interval is simply a slice through this function, which provides far more information as to what is compatible with the test model and its assumptions.

Furthermore, we can also plot every compatibility interval and its corresponding <span style="color:#d46c5b">[_**S-value**_](https://www.lesslikely.com/statistics/s-values/)</span> using the _**plotsint**_ function 

```{r fig.height=6, fig.width=7}
sfunction<-plotsint(intervalsdf)
sfunction
```

The graph above provides us with compatibility levels and the maximum amount of information against the effect sizes contained in the compatibility interval. 

# Simple Linear Models

We can also try this with other simple linear models. 

Let's simulate more normal data and fit a simple linear regression to it using ordinary least squares regression with the _**lm**_ function. 

```{r}

GroupA2<-rnorm(500)
GroupB2<-rnorm(500)

RandomData2<-data.frame(GroupA2, GroupB2)

model<-lm(GroupA2 ~ GroupB2, data=RandomData2)
summary(model)
confint(model)
```

We can see some of the basic statistics of our model including the 95% interval for our predictor (GroupB). Perhaps we want more information. Well we can do that! Using the _**geninterval**_ function in the <span style="color:#d46c5b">[**estimation**](https://github.com/Zadchow/estimation)</span> package, we can calculate several compatibility intervals for the regression coefficient and then plot the P-value and S-value functions. 
```{r fig.height=6, fig.width=7}
randomframe<-genintervals(model, "GroupB2")

plotpint(randomframe)
```
Here's our P-value function. 

```{r fig.height=6, fig.width=7}
s<-plotsint(randomframe)
s
```
Now our S-value function. 

We can also compare these functions to likelihood functions (also called support intervals), and we'll see that we get very similar results. We'll do this using the _**ProfileLikelihood**_ package. 

```{r include=FALSE}
library(ProfileLikelihood)
```


```{r}
xx <- profilelike.lm(formula = GroupA2 ~ 1, data=RandomData2, 
                     profile.theta="GroupB2",
                     lo.theta=-0.3, hi.theta=0.3, length=500)
```

```{r echo=FALSE}
profilelike.plot<-function (theta = theta, profile.lik.norm = profile.lik.norm, 
          round = 2) 
{
  mle <- round(max(theta[profile.lik.norm == max(profile.lik.norm)]), 
               round)
  theta1.x.p <- theta[profile.lik.norm >= (1/8)]
  theta2.x.p <- theta[profile.lik.norm >= (1/20)]
  theta3.x.p <- theta[profile.lik.norm >= (1/32)]
  theta.x.p.norm <- theta[profile.lik.norm >= 0.146]
  li.x.p.norm <- rep(0.146, length(theta.x.p.norm))
  li8.x.p <- rep(1/8, length(theta1.x.p))
  li20.x.p <- rep(1/20, length(theta2.x.p))
  li32.x.p <- rep(1/32, length(theta3.x.p))
  plot(theta, profile.lik.norm, type = "l", lty = 1, lwd = 2.5, 
       ylim = c(0, 1), xlim = c(min(theta), max(theta)), ylab = "", 
       xlab = expression(theta))
  lines(theta.x.p.norm, li.x.p.norm, lty = 1, lwd = 2.5, col = "#d46c5b")
  lines(theta1.x.p, li8.x.p, lty = 1, lwd = 2.5, col = "#239a98")
  lines(theta2.x.p, li20.x.p, lty = 1, lwd = 2.5, col = "salmon")
  lines(theta3.x.p, li32.x.p, lty = 1, lwd = 2.5, col = "grey50")
  abline(v = 0, lty = 2, lwd = 1.5, col = "gray")
  if (mle > 0) {
    pos = 0.15
  }
  else {
    pos = 0.8
  }
  text(theta[pos * length(theta)], 1, paste("Max at  ", round(max(theta[profile.lik.norm == 
                                                                          max(profile.lik.norm)]), round)), cex = 1, col = 1)
  text(theta[pos * length(theta)], 0.95, paste("1/6.8 LI (", 
                                               round(min(theta.x.p.norm), round), ",", round(max(theta.x.p.norm), 
                                                                                             round), ")"), cex = 1, col = "#d46c5b")
  text(theta[pos * length(theta)], 0.9, paste("1/8 LI (", 
                                              round(min(theta1.x.p), round), ",", round(max(theta1.x.p), 
                                                                                        round), ")"), cex = 1, col = "#239a98")
  text(theta[pos * length(theta)], 0.85, paste("1/20 LI (", 
                                               round(min(theta2.x.p), round), ",", round(max(theta2.x.p), 
                                                                                         round), ")"), cex = 1, col = "salmon")
  text(theta[pos * length(theta)], 0.8, paste("1/32 LI (", 
                                              round(min(theta3.x.p), round), ",", round(max(theta3.x.p), 
                                                                                        round), ")"), cex = 1, col = "grey50")
}
```

Now we plot our likelihood function and we can see what the maximum likelihood estimation is. Notice that it's practically similar to the interval in the S-value function with 0 bits of information against it and and the compatibility interval in the P-value function with a P-value of 1.  

```{r fig.height=6.5, fig.width=7}
profilelike.plot(theta=xx$theta, 
                 profile.lik.norm=xx$profile.lik.norm, round=3)
title(main = "Likelihood Function")
```

We’ve used a relatively easy example for this blog post, but the <span style="color:#d46c5b">[**estimation**](https://github.com/Zadchow/estimation)</span> package is also able to calculate compatibility functions for multiple regressions, logistic regressions, ANOVAs, and meta-analyses (that have been produced by the _**metafor**_ package). 

# Using Meta-Analysis Data

Here we present another quick example with a meta-analysis of simulated data. 

First, we simulate random data for two groups in two hypothetical studies

```{r}
GroupAData<-runif(20, min=0, max=100)
GroupAMean<-round(mean(GroupAData), digits=2)
GroupASD<-round(sd(GroupAData), digits=2)

GroupBData<-runif(20, min=0, max=100)
GroupBMean<-round(mean(GroupBData), digits=2)
GroupBSD<-round(sd(GroupBData), digits=2)

GroupCData<-runif(20, min=0, max=100)
GroupCMean<-round(mean(GroupCData), digits=2)
GroupCSD<-round(sd(GroupCData), digits=2)

GroupDData<-runif(20, min=0, max=100)
GroupDMean<-round(mean(GroupDData), digits=2)
GroupDSD<-round(sd(GroupDData), digits=2)
```


We can then quickly combine the data in a dataframe. 
```{r}
StudyName<-c("Study1", "Study2")
MeanTreatment<-c(GroupAMean, GroupCMean)
MeanControl<-c(GroupBMean, GroupDMean)
SDTreatment<-c(GroupASD, GroupCSD)
SDControl<-c(GroupBSD, GroupDSD)
NTreatment<-c(20,20)
NControl<-c(20,20)

metadf<-data.frame(StudyName, MeanTreatment, MeanControl, 
                   SDTreatment, SDControl, 
                   NTreatment, NControl)
```

Then, we'll use _**metafor**_ to calculate the standardized mean difference. 

```{r include=FALSE}
library(metafor)
```

```{r}
dat<-escalc(measure="SMD", 
            m1i=MeanTreatment, sd1i=SDTreatment, n1i=NTreatment,
            m2i=MeanControl, sd2i=SDControl, n2i=NControl, 
            data=metadf)
```

Next, we'll pool the data using a ~~fixed-effects~~ common-effects model

```{r}
res<-rma(yi, vi, data=dat, slab=paste(StudyName, sep=", "), 
         method="FE", digits=2)
```

Let's plot our results in a forest plot. I'm going to omit this code since it's a bit long. 

```{r echo=FALSE}
# .psort was required. obtained it from https://rdrr.io/cran/metafor/src/R/misc.func.hidden.r
.psort <- function(x,y) {
  
  ### t(apply(xy, 1, sort)) would be okay, but problematic if there are NAs;
  ### either they are removed completely (na.last=NA) or they are always put
  ### first/last (na.last=FALSE/TRUE); but we just want to leave the NAs in
  ### their position!
  
  if (is.null(x) || length(x) == 0) ### need to catch this
    return(NULL)
  
  if (missing(y)) {
    if (is.matrix(x)) {
      xy <- x
    } else {
      xy <- rbind(x) ### in case x is just a vector
    }
  } else {
    xy <- cbind(x,y)
  }
  
  n <- nrow(xy)
  
  for (i in seq_len(n)) {
    if (anyNA(xy[i,]))
      next
    xy[i,] <- sort(xy[i,])
  }
  
  colnames(xy) <- NULL
  
  return(xy)
  
}

# modified forest function for Zad, which includes "study.col" as an argument for the color of the study points
forest.zad <- function (x, annotate = TRUE, addfit = TRUE, addcred = FALSE, 
          showweights = FALSE, xlim, alim, clim, ylim, at, steps = 5, 
          level = x$level, refline = 0, digits = 2L, width, xlab, slab, 
          mlab, ilab, ilab.xpos, ilab.pos, order, transf, atransf, 
          targs, rows, efac = 1, pch = 15, psize, col, study.col, border, lty, 
          cex, cex.lab, cex.axis, annosym, ...) 
{
  if (!inherits(x, "rma")) 
    stop("Argument 'x' must be an object of class \"rma\".")
  if (inherits(x, "rma.ls")) 
    stop("Method not yet implemented for objects of class \"rma.ls\". Sorry!")
  na.act <- getOption("na.action")
  if (!is.element(na.act, c("na.omit", "na.exclude", "na.fail", 
                            "na.pass"))) 
    stop("Unknown 'na.action' specified under options().")
  if (missing(transf)) 
    transf <- FALSE
  if (missing(atransf)) 
    atransf <- FALSE
  transf.char <- deparse(substitute(transf))
  atransf.char <- deparse(substitute(atransf))
  if (is.function(transf) && is.function(atransf)) 
    stop("Use either 'transf' or 'atransf' to specify a transformation (not both).")
  if (missing(targs)) 
    targs <- NULL
  if (missing(at)) 
    at <- NULL
  if (missing(ilab)) 
    ilab <- NULL
  if (missing(ilab.xpos)) 
    ilab.xpos <- NULL
  if (missing(ilab.pos)) 
    ilab.pos <- NULL
  if (missing(order)) 
    order <- NULL
  if (missing(psize)) 
    psize <- NULL
  if (missing(cex)) 
    cex <- NULL
  if (missing(cex.lab)) 
    cex.lab <- NULL
  if (missing(cex.axis)) 
    cex.axis <- NULL
  if (x$int.only) {
    if (missing(col)) {
      col <- c("black", "gray50")
    }
    else {
      if (length(col) == 1L) 
        col <- c(col, "gray50")
    }
    if (missing(border)) 
      border <- "black"
  }
  else {
    if (missing(col)) 
      col <- "gray"
    if (missing(border)) 
      border <- "gray"
  }
  if (missing(lty)) {
    lty <- c("solid", "dotted", "solid")
  }
  else {
    if (length(lty) == 1L) 
      lty <- c(lty, "dotted", "solid")
    if (length(lty) == 2L) 
      lty <- c(lty, "solid")
  }
  if (length(efac) == 1L) 
    efac <- rep(efac, 3)
  if (length(efac) == 2L) 
    efac <- c(efac[1], efac[1], efac[2])
  if (missing(annosym)) 
    annosym <- c(" [", ", ", "]")
  if (length(annosym) != 3) 
    stop("Argument 'annosym' must be a vector of length 3.")
  measure <- x$measure
  if (inherits(x, "rma.glmm") && showweights) 
    stop("Option 'showweights=TRUE' currently not possible for 'rma.glmm' objects. Sorry!")
  if (length(digits) == 1L) 
    digits <- c(digits, digits)
  level <- ifelse(level > 1, (100 - level)/100, ifelse(level > 
                                                         0.5, 1 - level, level))
  yi <- x$yi.f
  vi <- x$vi.f
  X <- x$X.f
  k <- length(yi)
  if (missing(slab)) {
    if (x$slab.null) {
      slab <- paste("Study", x$slab)
    }
    else {
      slab <- x$slab
    }
  }
  else {
    if (length(slab) == 1 && is.na(slab)) 
      slab <- rep("", k)
  }
  if (length(yi) != length(slab)) 
    stop("Number of outcomes does not correspond to the length of the 'slab' argument.")
  if (is.null(dim(ilab))) 
    ilab <- cbind(ilab)
  if (length(pch) == 1L) 
    pch <- rep(pch, k)
  if (length(pch) != length(yi)) 
    stop("Number of outcomes does not correspond to the length of the 'pch' argument.")
  options(na.action = "na.pass")
  if (x$int.only) {
    pred <- fitted(x)
    pred.ci.lb <- rep(NA_real_, k)
    pred.ci.ub <- rep(NA_real_, k)
  }
  else {
    temp <- predict(x, level = level)
    pred <- temp$pred
    if (addcred) {
      pred.ci.lb <- temp$cr.lb
      pred.ci.ub <- temp$cr.ub
    }
    else {
      pred.ci.lb <- temp$ci.lb
      pred.ci.ub <- temp$ci.ub
    }
  }
  if (inherits(x, "rma.glmm")) {
    weights <- NULL
  }
  else {
    weights <- weights(x)
  }
  options(na.action = na.act)
  if (!is.null(psize)) {
    if (length(psize) == 1L) 
      psize <- rep(psize, k)
    if (length(psize) != length(yi)) 
      stop("Number of outcomes does not correspond to the length of the 'psize' argument.")
  }
  if (!is.null(order)) {
    if (is.character(order)) {
      if (length(order) != 1) 
        stop("Incorrect length of 'order' argument.")
      if (order == "obs") 
        sort.vec <- order(yi)
      if (order == "fit") 
        sort.vec <- order(pred)
      if (order == "prec") 
        sort.vec <- order(vi, yi)
      if (order == "resid") 
        sort.vec <- order(yi - pred, yi)
      if (order == "rstandard") 
        sort.vec <- order(rstandard(x)$z, yi)
      if (order == "abs.resid") 
        sort.vec <- order(abs(yi - pred), yi)
      if (order == "abs.rstandard") 
        sort.vec <- order(abs(rstandard(x)$z), yi)
    }
    else {
      sort.vec <- order
    }
    yi <- yi[sort.vec]
    vi <- vi[sort.vec]
    X <- X[sort.vec, , drop = FALSE]
    slab <- slab[sort.vec]
    ilab <- ilab[sort.vec, , drop = FALSE]
    pred <- pred[sort.vec]
    pred.ci.lb <- pred.ci.lb[sort.vec]
    pred.ci.ub <- pred.ci.ub[sort.vec]
    weights <- weights[sort.vec]
    pch <- pch[sort.vec]
    psize <- psize[sort.vec]
  }
  k <- length(yi)
  if (missing(rows)) {
    rows <- k:1
  }
  else {
    if (length(rows) == 1L) {
      rows <- rows:(rows - k + 1)
    }
  }
  if (length(rows) != length(yi)) 
    stop("Number of outcomes does not correspond to the length of the 'rows' argument.")
  yi <- yi[k:1]
  vi <- vi[k:1]
  X <- X[k:1, , drop = FALSE]
  slab <- slab[k:1]
  ilab <- ilab[k:1, , drop = FALSE]
  pred <- pred[k:1]
  pred.ci.lb <- pred.ci.lb[k:1]
  pred.ci.ub <- pred.ci.ub[k:1]
  weights <- weights[k:1]
  pch <- pch[k:1]
  psize <- psize[k:1]
  rows <- rows[k:1]
  yiviX.na <- is.na(yi) | is.na(vi) | apply(is.na(X), 1, any)
  if (any(yiviX.na)) {
    not.na <- !yiviX.na
    if (na.act == "na.omit") {
      yi <- yi[not.na]
      vi <- vi[not.na]
      X <- X[not.na, , drop = FALSE]
      slab <- slab[not.na]
      ilab <- ilab[not.na, , drop = FALSE]
      pred <- pred[not.na]
      pred.ci.lb <- pred.ci.lb[not.na]
      pred.ci.ub <- pred.ci.ub[not.na]
      weights <- weights[not.na]
      pch <- pch[not.na]
      psize <- psize[not.na]
      rows.new <- rows
      rows.na <- rows[!not.na]
      for (j in seq_len(length(rows.na))) {
        rows.new[rows >= rows.na[j]] <- rows.new[rows >= 
                                                   rows.na[j]] - 1
      }
      rows <- rows.new[not.na]
    }
    if (na.act == "na.fail") 
      stop("Missing values in results.")
  }
  k <- length(yi)
  ci.lb <- yi - qnorm(level/2, lower.tail = FALSE) * sqrt(vi)
  ci.ub <- yi + qnorm(level/2, lower.tail = FALSE) * sqrt(vi)
  if (is.function(transf)) {
    if (is.null(targs)) {
      yi <- sapply(yi, transf)
      ci.lb <- sapply(ci.lb, transf)
      ci.ub <- sapply(ci.ub, transf)
      pred <- sapply(pred, transf)
      pred.ci.lb <- sapply(pred.ci.lb, transf)
      pred.ci.ub <- sapply(pred.ci.ub, transf)
    }
    else {
      yi <- sapply(yi, transf, targs)
      ci.lb <- sapply(ci.lb, transf, targs)
      ci.ub <- sapply(ci.ub, transf, targs)
      pred <- sapply(pred, transf, targs)
      pred.ci.lb <- sapply(pred.ci.lb, transf, targs)
      pred.ci.ub <- sapply(pred.ci.ub, transf, targs)
    }
  }
  tmp <- .psort(ci.lb, ci.ub)
  ci.lb <- tmp[, 1]
  ci.ub <- tmp[, 2]
  tmp <- .psort(pred.ci.lb, pred.ci.ub)
  pred.ci.lb <- tmp[, 1]
  pred.ci.ub <- tmp[, 2]
  if (!missing(clim)) {
    clim <- sort(clim)
    if (length(clim) != 2L) 
      stop("Argument 'clim' must be of length 2.")
    ci.lb[ci.lb < clim[1]] <- clim[1]
    ci.ub[ci.ub > clim[2]] <- clim[2]
    pred.ci.lb[pred.ci.lb < clim[1]] <- clim[1]
    pred.ci.ub[pred.ci.ub > clim[2]] <- clim[2]
  }
  if (is.null(psize)) {
    if (is.null(weights)) {
      if (any(vi <= 0, na.rm = TRUE)) {
        psize <- rep(1, k)
      }
      else {
        wi <- 1/sqrt(vi)
        psize <- wi/sum(wi, na.rm = TRUE)
        psize <- (psize - min(psize, na.rm = TRUE))/(max(psize, 
                                                         na.rm = TRUE) - min(psize, na.rm = TRUE))
        psize <- (psize * 1) + 0.5
        if (all(is.na(psize))) 
          psize <- rep(1, k)
      }
    }
    else {
      wi <- weights
      psize <- wi/sum(wi, na.rm = TRUE)
      psize <- (psize - min(psize, na.rm = TRUE))/(max(psize, 
                                                       na.rm = TRUE) - min(psize, na.rm = TRUE))
      psize <- (psize * 1) + 0.5
      if (all(is.na(psize))) 
        psize <- rep(1, k)
    }
  }
  rng <- max(ci.ub, na.rm = TRUE) - min(ci.lb, na.rm = TRUE)
  if (annotate) {
    if (showweights) {
      plot.multp.l <- 2
      plot.multp.r <- 2
    }
    else {
      plot.multp.l <- 1.2
      plot.multp.r <- 1.2
    }
  }
  else {
    plot.multp.l <- 1.2
    plot.multp.r <- 0.4
  }
  if (missing(xlim)) {
    xlim <- c(min(ci.lb, na.rm = TRUE) - rng * plot.multp.l, 
              max(ci.ub, na.rm = TRUE) + rng * plot.multp.r)
    xlim <- round(xlim, digits[2])
  }
  alim.spec <- TRUE
  if (missing(alim)) {
    if (is.null(at)) {
      alim <- range(pretty(x = c(min(ci.lb, na.rm = TRUE), 
                                 max(ci.ub, na.rm = TRUE)), n = steps - 1))
      alim.spec <- FALSE
    }
    else {
      alim <- range(at)
    }
  }
  alim <- sort(alim)
  xlim <- sort(xlim)
  if (xlim[1] > min(yi, na.rm = TRUE)) {
    xlim[1] <- min(yi, na.rm = TRUE)
  }
  if (xlim[2] < max(yi, na.rm = TRUE)) {
    xlim[2] <- max(yi, na.rm = TRUE)
  }
  if (alim[1] < xlim[1]) {
    xlim[1] <- alim[1]
  }
  if (alim[2] > xlim[2]) {
    xlim[2] <- alim[2]
  }
  if (missing(ylim)) {
    if (x$int.only && addfit) {
      ylim <- c(-1.5, k + 3)
    }
    else {
      ylim <- c(0.5, k + 3)
    }
  }
  else {
    ylim <- sort(ylim)
  }
  if (is.null(at)) {
    if (alim.spec) {
      at <- seq(from = alim[1], to = alim[2], length.out = steps)
    }
    else {
      at <- pretty(x = c(min(ci.lb, na.rm = TRUE), max(ci.ub, 
                                                       na.rm = TRUE)), n = steps - 1)
    }
  }
  else {
    at[at < alim[1]] <- alim[1]
    at[at > alim[2]] <- alim[2]
    at <- unique(at)
  }
  at.lab <- at
  if (is.function(atransf)) {
    if (is.null(targs)) {
      at.lab <- formatC(sapply(at.lab, atransf), digits = digits[2], 
                        format = "f", drop0trailing = ifelse(class(digits) == 
                                                               "integer", TRUE, FALSE))
    }
    else {
      at.lab <- formatC(sapply(at.lab, atransf, targs), 
                        digits = digits[2], format = "f", drop0trailing = ifelse(class(digits) == 
                                                                                   "integer", TRUE, FALSE))
    }
  }
  else {
    at.lab <- formatC(at.lab, digits = digits[2], format = "f", 
                      drop0trailing = ifelse(class(digits) == "integer", 
                                             TRUE, FALSE))
  }
  par.mar <- par("mar")
  par.mar.adj <- par.mar - c(0, 3, 1, 1)
  par.mar.adj[par.mar.adj < 0] <- 0
  par(mar = par.mar.adj)
  on.exit(par(mar = par.mar))
  plot(NA, NA, xlim = xlim, ylim = ylim, xlab = "", ylab = "", 
       yaxt = "n", xaxt = "n", xaxs = "i", bty = "n", ...)
  abline(h = ylim[2] - 2, lty = lty[3], ...)
  if (is.numeric(refline)) 
    segments(refline, ylim[1] - 5, refline, ylim[2] - 2, 
             lty = "dotted", ...)
  par.usr <- par("usr")
  height <- par.usr[4] - par.usr[3]
  if (is.null(cex)) {
    lheight <- strheight("O")
    cex.adj <- ifelse(k * lheight > height * 0.8, height/(1.25 * 
                                                            k * lheight), 1)
  }
  if (is.null(cex)) {
    cex <- par("cex") * cex.adj
  }
  else {
    if (is.null(cex.lab)) 
      cex.lab <- cex
    if (is.null(cex.axis)) 
      cex.axis <- cex
  }
  if (is.null(cex.lab)) 
    cex.lab <- par("cex") * cex.adj
  if (is.null(cex.axis)) 
    cex.axis <- par("cex") * cex.adj
  if (addfit && !x$int.only) {
    for (i in seq_len(k)) {
      if (is.na(pred[i])) 
        next
      polygon(x = c(max(pred.ci.lb[i], alim[1]), pred[i], 
                    min(pred.ci.ub[i], alim[2]), pred[i]), y = c(rows[i], 
                                                                 rows[i] + (height/100) * cex * efac[3], rows[i], 
                                                                 rows[i] - (height/100) * cex * efac[3]), col = col, 
              border = border, ...)
    }
  }
  if (addfit && x$int.only) {
    if (inherits(x, "rma.mv") && x$withG && x$tau2s > 1) {
      if (!is.logical(addcred)) {
        if (length(addcred) == 1) 
          addcred <- c(addcred, addcred)
        temp <- predict(x, level = level, tau2.levels = addcred[1], 
                        gamma2.levels = addcred[2])
        addcred <- TRUE
      }
      else {
        if (addcred) {
          stop("Need to specify the level of the inner factor(s) via the 'addcred' argument.")
        }
        else {
          temp <- predict(x, level = level, tau2.levels = 1, 
                          gamma2.levels = 1)
        }
      }
    }
    else {
      temp <- predict(x, level = level)
    }
    beta <- temp$pred
    beta.ci.lb <- temp$ci.lb
    beta.ci.ub <- temp$ci.ub
    beta.cr.lb <- temp$cr.lb
    beta.cr.ub <- temp$cr.ub
    if (is.function(transf)) {
      if (is.null(targs)) {
        beta <- sapply(beta, transf)
        beta.ci.lb <- sapply(beta.ci.lb, transf)
        beta.ci.ub <- sapply(beta.ci.ub, transf)
        beta.cr.lb <- sapply(beta.cr.lb, transf)
        beta.cr.ub <- sapply(beta.cr.ub, transf)
      }
      else {
        beta <- sapply(beta, transf, targs)
        beta.ci.lb <- sapply(beta.ci.lb, transf, targs)
        beta.ci.ub <- sapply(beta.ci.ub, transf, targs)
        beta.cr.lb <- sapply(beta.cr.lb, transf, targs)
        beta.cr.ub <- sapply(beta.cr.ub, transf, targs)
      }
    }
    tmp <- .psort(beta.ci.lb, beta.ci.ub)
    beta.ci.lb <- tmp[, 1]
    beta.ci.ub <- tmp[, 2]
    tmp <- .psort(beta.cr.lb, beta.cr.ub)
    beta.cr.lb <- tmp[, 1]
    beta.cr.ub <- tmp[, 2]
    if (!missing(clim)) {
      beta.ci.lb[beta.ci.lb < clim[1]] <- clim[1]
      beta.ci.ub[beta.ci.ub > clim[2]] <- clim[2]
      beta.cr.lb[beta.cr.lb < clim[1]] <- clim[1]
      beta.cr.ub[beta.cr.ub > clim[2]] <- clim[2]
    }
    if (x$method != "FE" && addcred) {
      segments(max(beta.cr.lb, alim[1]), -1, min(beta.cr.ub, 
                                                 alim[2]), -1, lty = lty[2], col = col[2], ...)
      if (beta.cr.lb >= alim[1]) {
        segments(beta.cr.lb, -1 - (height/150) * cex * 
                   efac[1], beta.cr.lb, -1 + (height/150) * cex * 
                   efac[1], col = col[2], ...)
      }
      else {
        polygon(x = c(alim[1], alim[1] + (1.4/100) * 
                        cex * (xlim[2] - xlim[1]), alim[1] + (1.4/100) * 
                        cex * (xlim[2] - xlim[1]), alim[1]), y = c(-1, 
                                                                   -1 + (height/150) * cex * efac[2], -1 - (height/150) * 
                                                                     cex * efac[2], -1), col = col[2], border = col[2], 
                ...)
      }
      if (beta.cr.ub <= alim[2]) {
        segments(beta.cr.ub, -1 - (height/150) * cex * 
                   efac[1], beta.cr.ub, -1 + (height/150) * cex * 
                   efac[1], col = col[2], ...)
      }
      else {
        polygon(x = c(alim[2], alim[2] - (1.4/100) * 
                        cex * (xlim[2] - xlim[1]), alim[2] - (1.4/100) * 
                        cex * (xlim[2] - xlim[1]), alim[2]), y = c(-1, 
                                                                   -1 + (height/150) * cex * efac[2], -1 - (height/150) * 
                                                                     cex * efac[2], -1), col = col[2], border = col[2], 
                ...)
      }
    }
    polygon(x = c(beta.ci.lb, beta, beta.ci.ub, beta), y = c(-1, 
                                                             -1 + (height/100) * cex * efac[3], -1, -1 - (height/100) * 
                                                               cex * efac[3]), col = col[1], border = border, 
            ...)
    if (missing(mlab)) 
      mlab <- ifelse((x$method == "FE"), "FE Model", "RE Model")
    text(xlim[1], -1, mlab, pos = 4, cex = cex, ...)
  }
  axis(side = 1, at = at, labels = at.lab, cex.axis = cex.axis, 
       ...)
  if (missing(xlab)) 
    xlab <- .setlab(measure, transf.char, atransf.char, gentype = 1)
  mtext(xlab, side = 1, at = min(at) + (max(at) - min(at))/2, 
        line = par("mgp")[1] - 0.5, cex = cex.lab, ...)
  for (i in seq_len(k)) {
    if (is.na(yi[i]) || is.na(vi[i])) 
      next
    if (ci.lb[i] >= alim[2]) {
      polygon(x = c(alim[2], alim[2] - (1.4/100) * cex * 
                      (xlim[2] - xlim[1]), alim[2] - (1.4/100) * cex * 
                      (xlim[2] - xlim[1]), alim[2]), y = c(rows[i], 
                                                           rows[i] + (height/150) * cex * efac[2], rows[i] - 
                                                             (height/150) * cex * efac[2], rows[i]), col = "black", 
              ...)
      next
    }
    if (ci.ub[i] <= alim[1]) {
      polygon(x = c(alim[1], alim[1] + (1.4/100) * cex * 
                      (xlim[2] - xlim[1]), alim[1] + (1.4/100) * cex * 
                      (xlim[2] - xlim[1]), alim[1]), y = c(rows[i], 
                                                           rows[i] + (height/150) * cex * efac[2], rows[i] - 
                                                             (height/150) * cex * efac[2], rows[i]), col = "black", 
              ...)
      next
    }
    segments(max(ci.lb[i], alim[1]), rows[i], min(ci.ub[i], 
                                                  alim[2]), rows[i], lty = lty[1], ...)
    if (ci.lb[i] >= alim[1]) {
      segments(ci.lb[i], rows[i] - (height/150) * cex * 
                 efac[1], ci.lb[i], rows[i] + (height/150) * cex * 
                 efac[1], ...)
    }
    else {
      polygon(x = c(alim[1], alim[1] + (1.4/100) * cex * 
                      (xlim[2] - xlim[1]), alim[1] + (1.4/100) * cex * 
                      (xlim[2] - xlim[1]), alim[1]), y = c(rows[i], 
                                                           rows[i] + (height/150) * cex * efac[2], rows[i] - 
                                                             (height/150) * cex * efac[2], rows[i]), col = "black", 
              ...)
    }
    if (ci.ub[i] <= alim[2]) {
      segments(ci.ub[i], rows[i] - (height/150) * cex * 
                 efac[1], ci.ub[i], rows[i] + (height/150) * cex * 
                 efac[1], ...)
    }
    else {
      polygon(x = c(alim[2], alim[2] - (1.4/100) * cex * 
                      (xlim[2] - xlim[1]), alim[2] - (1.4/100) * cex * 
                      (xlim[2] - xlim[1]), alim[2]), y = c(rows[i], 
                                                           rows[i] + (height/150) * cex * efac[2], rows[i] - 
                                                             (height/150) * cex * efac[2], rows[i]), col = "black", 
              ...)
    }
  }
  text(xlim[1], rows, slab, pos = 4, cex = cex, ...)
  if (!is.null(ilab)) {
    if (is.null(ilab.xpos)) 
      stop("Must specify 'ilab.xpos' argument when adding information with 'ilab'.")
    if (length(ilab.xpos) != ncol(ilab)) 
      stop(paste0("Number of 'ilab' columns (", ncol(ilab), 
                  ") does not match length of 'ilab.xpos' argument (", 
                  length(ilab.xpos), ")."))
    if (!is.null(ilab.pos) && length(ilab.pos) == 1) 
      ilab.pos <- rep(ilab.pos, ncol(ilab))
    for (l in seq_len(ncol(ilab))) {
      text(ilab.xpos[l], rows, ilab[, l], pos = ilab.pos[l], 
           cex = cex, ...)
    }
  }
  if (annotate) {
    if (is.function(atransf)) {
      if (is.null(targs)) {
        if (addfit && x$int.only) {
          annotext <- cbind(sapply(c(yi, beta), atransf), 
                            sapply(c(ci.lb, beta.ci.lb), atransf), sapply(c(ci.ub, 
                                                                            beta.ci.ub), atransf))
        }
        else {
          annotext <- cbind(sapply(yi, atransf), sapply(ci.lb, 
                                                        atransf), sapply(ci.ub, atransf))
        }
      }
      else {
        if (addfit && x$int.only) {
          annotext <- cbind(sapply(c(yi, beta), atransf, 
                                   targs), sapply(c(ci.lb, beta.ci.lb), atransf, 
                                                  targs), sapply(c(ci.ub, beta.ci.ub), atransf, 
                                                                 targs))
        }
        else {
          annotext <- cbind(sapply(yi, atransf, targs), 
                            sapply(ci.lb, atransf, targs), sapply(ci.ub, 
                                                                  atransf, targs))
        }
      }
      tmp <- .psort(annotext[, 2:3])
      annotext[, 2:3] <- tmp
    }
    else {
      if (addfit && x$int.only) {
        annotext <- cbind(c(yi, beta), c(ci.lb, beta.ci.lb), 
                          c(ci.ub, beta.ci.ub))
      }
      else {
        annotext <- cbind(yi, ci.lb, ci.ub)
      }
    }
    if (showweights) {
      if (addfit && x$int.only) {
        annotext <- cbind(c(weights, 100), annotext)
      }
      else {
        annotext <- cbind(weights, annotext)
      }
    }
    annotext <- formatC(annotext, format = "f", digits = digits[1])
    if (missing(width)) {
      width <- apply(annotext, 2, function(x) max(nchar(x)))
    }
    else {
      if (length(width) == 1L) 
        width <- rep(width, ncol(annotext))
    }
    for (j in seq_len(ncol(annotext))) {
      annotext[, j] <- formatC(annotext[, j], width = width[j])
    }
    if (showweights) {
      annotext <- cbind(annotext[, 1], "%   ", annotext[, 
                                                        2], annosym[1], annotext[, 3], annosym[2], annotext[, 
                                                                                                            4], annosym[3])
    }
    else {
      annotext <- cbind(annotext[, 1], annosym[1], annotext[, 
                                                            2], annosym[2], annotext[, 3], annosym[3])
    }
    annotext <- apply(annotext, 1, paste, collapse = "")
    if (addfit && x$int.only) {
      text(x = xlim[2], c(rows, -1), labels = annotext, 
           pos = 2, cex = cex, ...)
    }
    else {
      text(x = xlim[2], rows, labels = annotext, pos = 2, 
           cex = cex, ...)
    }
  }
  for (i in seq_len(k)) {
    if (is.na(yi[i])) 
      next
    if (yi[i] >= alim[1] && yi[i] <= alim[2]) 
      points(yi[i], rows[i], pch = pch[i], cex = cex * 
               psize[i], col = study.col, ...) # MODIFIED
  }
  if (x$int.only && addfit) 
    abline(h = 0, lty = lty[3], ...)
  res <- list(xlim = par("usr")[1:2], alim = alim, at = at, 
              ylim = ylim, rows = rows, cex = cex, cex.lab = cex.lab, 
              cex.axis = cex.axis)
  invisible(res)
}

```

```{r echo=FALSE, fig.height=6, fig.width=7}
forest.zad(res, xlim=c(-16, 6), at=(c(-1, -0.5, 0.5, 1)),
       ilab=cbind(dat$MeanTreatment, dat$MeanControl, dat$SDTreatment, dat$SDControl),
       ilab.xpos=c(-9.5,-8,-6,-4.5), cex=0.90, ylim=c(-1, 5),
       xlab="Standardized Mean Difference", mlab="", psize=1.9, pch=16, study.col="#d46c5b", col="#d46c5b") 

## Heterogeneity 

text(-16, -1, pos=4, cex=0.95, bquote(paste("Fixed Effects (Q = ",
                                            .(formatC(res$QE, digits=2, format="f")), ", df = ", .(res$k - res$p),
                                            ", p = ", .(formatC(res$QEp, digits=2, format="f")), "; ", I^2, " = ",
                                            .(formatC(res$I2, digits=1, format="f")), "%)")))

op <- par(cex=0.90, font=4)
### Bold Font
par(font=2)

### Columns
text(c(-9.5,-8,-6,-4.5), 2.5, c("Treat", "Contrl", "Treat", "Contrl"))
text(c(-8.75,-5.25),     3.5, c("Effects", "Variance"))
text(-16,                3.5, "Study Name",  pos=4)
text(6,                  3.5, "Standardized Mean Difference [95% CI]", pos=2)
```

Take a look at the pooled summary effect and its interval. Keep it in mind as we move onto constructing a compatibility function. 

We can now take the object produced by the meta-analysis and calculate a P-value and S-value function with it to see the full spectrum of effect sizes compatible with the test model at every level. We'll use the _**metainterval**_ function to do this. 

```{r}
metaf<-metaintervals(res)
```

Now that we have our dataframe with every computed interval, we can plot the functions.

```{r fig.height=6, fig.width=7}
plotpint(metaf)
```

And our S-value function

```{r fig.height=6, fig.width=7}
plotsint(metaf)
```

Compare the span of these functions and the information they provide to the compatibility interval provided by the forest plot. We are now no longer limited to interpreting an arbitrarily chosen interval by mindless analytic decisions often built into statistical packages by default.

>"_Statistical software enables and promotes cargo-cult statistics. Marketing and adoption of statistical software are driven by ease of use and the range of statistical routines the software implements. Offering complex and “modern” methods provides a competitive advantage. And some disciplines have in effect standardised on particular statistical software, often proprietary software_.
>
>_Statistical software does not help you know what to compute, nor how to interpret the result. It does not offer to explain the assumptions behind methods, nor does it flag delicate or dubious assumptions. It does not warn you about multiplicity or p-hacking. It does not check whether you picked the hypothesis or analysis after looking at the data, nor track the number of analyses you tried before arriving at the one you sought to publish – another form of multiplicity. The more “powerful” and “user-friendly” the software is, the more it invites cargo-cult statistics_."
>
>`r tufte::quote_footer('- Stark & Saltelli, 2018')`


# References 

1. Stark PB, Saltelli A. Cargo-cult statistics and scientific crisis. Significance. 2018;15(4):40-43. 
2. Poole C. Beyond the confidence interval. Am J Public Health. 1987;77(2):195-199. 
3. Sullivan KM, Foster DA. Use of the confidence interval function. Epidemiology. 1990;1(1):39-42. 
4. Rothman KJ, Greenland S, Lash TL, Others. Modern epidemiology. 2008. 
